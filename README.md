# Onyxia

**GPU compute shader runtime for ONNX models.** Compiles ONNX operator graphs into GPU compute shaders and executes them via `wgpu`.

## Architecture

```
ONNX Model â†’ onyxia-onnx â†’ onyxia-planner â†’ onyxia-runtime â†’ GPU Execution
  (.onnx)    (parse â†’ Graph)  (naga::Module    (wgpu pipelines   (results)
                               shaders)         + dispatch)
```

- **onyxia-onnx**: Parse ONNX protobuf into a stable Graph API
- **onyxia-planner**: Kernel-based shape inference and compilation into execution plans with pre-compiled shaders
- **onyxia-runtime**: Execute plans on GPU hardware via wgpu
- **onyxia-cli**: Command-line tools for testing and debugging

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed design documentation.

## Current Status

**The end-to-end pipeline is working** â€” from ONNX parsing through GPU execution:

```
âœ… ONNX Model â†’ Parser â†’ Graph
âœ… Graph â†’ Planner â†’ ExecutionPlan
âœ… ExecutionPlan â†’ Runtime â†’ GPU execution
âœ… GPU outputs â†’ CPU tensors
```

### What Works

- âœ… **ONNX parsing** with stable Graph API
- âœ… **Kernel-based shape inference** â€” three-phase: dynamic dim substitution â†’ forward inference with value propagation â†’ static-only planning
- âœ… **DOT graph visualization** (full, layers, summary views)
- âœ… **Extensible kernel system** â€” users add operations via `OpKernel` trait
- âœ… **Shader compilation** â€” WGSL â†’ `naga::Module` via naga_oil at plan time
- âœ… **Dynamic dimension resolution** at plan time
- âœ… **GPU execution** with buffer management and compute dispatch
- âœ… **End-to-end pipeline** verified
- âœ… **101 tests passing**, 22 GPU tests skipped in CI

### Built-in Kernels

| Kernel | ONNX Op | Category |
|--------|---------|----------|
| `AddKernel` | Add | Elementwise |
| `SubKernel` | Sub | Elementwise |
| `MulKernel` | Mul | Elementwise |
| `GeluKernel` | Gelu | Activation |
| `RmsNormKernel` | SimplifiedLayerNormalization | Normalization |
| `MatMulF32Kernel` | MatMul | Matrix multiplication |
| `MatMulNBitsKernel` | MatMulNBits | Quantized matmul |
| `CastKernel` | Cast | Type conversion |
| `ConstantKernel` | Constant | Metadata |
| `ShapeKernel` | Shape | Metadata |
| `ReshapeKernel` | Reshape | Shape manipulation |
| `UnsqueezeKernel` | Unsqueeze | Shape manipulation |
| `TransposeKernel` | Transpose | Shape manipulation |
| `ConcatKernel` | Concat | Shape manipulation |
| `GatherKernel` | Gather | Indexing |
| `ReduceMeanKernel` | ReduceMean | Reduction |
| `ReduceSumKernel` | ReduceSum | Reduction |
| `RotaryEmbeddingKernel` | RotaryEmbedding | Attention |
| `GroupQueryAttentionKernel` | GroupQueryAttention | Attention |

### What's Next

- ðŸ”œ More kernels for broader ONNX operation coverage
- ðŸ”œ Quantized model support â€” 4-bit, 8-bit via `MatMulNBits`
- ðŸ”œ KV cache management for efficient LLM generation
- ðŸ”œ Performance optimizations (fusion, tiling, memory pooling)
- ðŸ”œ Numerical validation against ONNX Runtime

## Usage

### Adding Custom Operations

```rust
use onyxia_planner::{OpKernel, InferenceContext, TensorValue, PlanContext, Step, KernelRegistry, compile};

struct MyCustomKernel;

impl OpKernel for MyCustomKernel {
    fn name(&self) -> &str { "MyCustomOp" }
    
    fn infer_output_shapes(
        &self,
        ctx: &InferenceContext<'_>,
    ) -> onyxia_planner::Result<Vec<TensorShape>> {
        // Define shape inference logic for this operation
        Ok(vec![ctx.input_shapes[0].clone()])
    }
    
    fn try_fold(
        &self,
        ctx: &InferenceContext<'_>,
    ) -> onyxia_planner::Result<Vec<Option<TensorValue>>> {
        // Optional: implement constant folding for compile-time evaluation
        Ok(vec![None])
    }
    
    fn plan(&self, ctx: &mut PlanContext<'_>) -> onyxia_planner::Result<Vec<Step>> {
        // Compile shader, set up bindings, return steps
        todo!()
    }
}

// Register and compile
let mut registry = KernelRegistry::with_defaults();
registry.register("MyCustomOp", Box::new(MyCustomKernel));
let plan = compile(&graph, &registry, &dynamic_dimensions)?;
```

### Running a Model

```rust
use onyxia_onnx::load_model;
use onyxia_planner::{compile, KernelRegistry};
use onyxia_runtime::{Runtime, Tensor};
use std::collections::HashMap;

#[pollster::main]
async fn main() -> anyhow::Result<()> {
    // Parse ONNX model
    let graph = load_model("model.onnx")?;

    // Compile to execution plan
    let registry = KernelRegistry::with_defaults();
    let dynamic_dimensions = HashMap::from([
        ("batch".to_string(), 1),
        ("sequence".to_string(), 512),
    ]);
    let plan = compile(&graph, &registry, &dynamic_dimensions)?;

    // Execute on GPU
    let runtime = Runtime::new().await?;
    let mut executor = runtime.load_model(plan).await?;

    let input = Tensor::from_vec(vec![1.0f32, 2.0, 3.0, 4.0], &[1, 4]);
    let outputs = executor.run(&[("input", input)])?;

    println!("Output: {:?}", outputs["output"].to_vec::<f32>()?);
    Ok(())
}
```

### Inspecting Models (CLI)

```bash
# Parse and analyze model structure
cargo run --bin onyxia -- inspect models/gemma-3-270m-it-ONNX/onnx/model_q4.onnx

# Generate DOT visualization
cargo run --bin onyxia -- dot models/gemma-3-270m-it-ONNX/onnx/model_q4.onnx \
  -o model.dot -s summary

# Convert to PNG (requires Graphviz)
dot -Tpng model.dot -o model.png
```

## Prerequisites

### Protocol Buffers Compiler (`protoc`)

Required for building the ONNX parser. Install via your package manager:

- **Windows (winget)**: `winget install protobuf`
- **Windows (Chocolatey)**: `choco install protoc`
- **macOS**: `brew install protobuf`
- **Linux (apt)**: `apt install protobuf-compiler`
- **Linux (dnf)**: `dnf install protobuf-compiler`

See [protobuf installation guide](https://protobuf.dev/installation/#package-manager) for more options.

## Building

```bash
cargo build
```

## Testing

We use [nextest](https://nexte.st/) as our test runner:

```bash
cargo nextest run
```

GPU-dependent tests are marked `#[ignore]` and can be run with:

```bash
cargo nextest run --run-ignored all
```

## Crates

| Crate | Description |
|-------|-------------|
| `onyxia-onnx` | ONNX protobuf parser, Graph API |
| `onyxia-planner` | Kernel-based shape inference and execution plan compiler |
| `onyxia-runtime` | GPU executor via wgpu |
| `onyxia-cli` | CLI tools for model inspection and DOT export |

## Example Models

The `models/` directory contains sample ONNX models for testing:

- **Gemma 3 270m** (quantized LLM): `models/gemma-3-270m-it-ONNX/onnx/model_q4.onnx`
  - 18 transformer layers, 4 attention heads, vocab size 262K
  - Uses `MatMulNBits` (4-bit quantized weights), `GroupQueryAttention`, `RotaryEmbedding`

## Development Roadmap

See [ARCHITECTURE.md](ARCHITECTURE.md) for the full development plan:

- âœ… Phase 1: Graph and Parser Foundation
- âœ… Phase 2: Planner and Kernel System
- âœ… Phase 3: Runtime Execution
- ðŸ”œ Phase 4: Quantization Support
- ðŸ”œ Phase 5: Attention and KV Cache
- ðŸ”œ Phase 6: Optimizations
- ðŸ”œ Phase 7: Polish and Advanced Features

## License

MIT OR Apache-2.0
